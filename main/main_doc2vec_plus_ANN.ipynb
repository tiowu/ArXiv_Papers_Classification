{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, CosineSimilarity, F1Score\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#from method_main import *\n",
    "#from method_json import *\n",
    "#from main_ANN_balanced_doc2vec_copy import *\n",
    "#from main_ANN_TW_meta import ANN_model_training\n",
    "\n",
    "#dict = read_category_csv_dict(\"ref_docu/CategoryGroupIndices.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using arXiv major huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of minor train set rows: 108,696\n",
      "\n",
      "Total number of minor test set rows: 27,178\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nJSON_10 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/major_train_processed.json\"\\nJSON_11 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/major_test_processed.json\"\\n\\ndf_major_train = pd.read_json(JSON_10, lines = \"True\")\\ndf_major_test = pd.read_json(JSON_11, lines = \"True\")\\nformatted_num_rows = \"{:,}\".format(len(df_major_train))\\nprint(f\"Total number of major train set rows: {formatted_num_rows}\\n\")\\nformatted_num_rows = \"{:,}\".format(len(df_major_test))\\nprint(f\"Total number of major test set rows: {formatted_num_rows}\\n\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON_8 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_train_processed.json\"\n",
    "JSON_9 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_test_processed.json\"\n",
    "\n",
    "\n",
    "df_minor_train = pd.read_json(JSON_8, lines = \"True\")\n",
    "df_minor_test = pd.read_json(JSON_9, lines = \"True\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_minor_train))\n",
    "print(f\"Total number of minor train set rows: {formatted_num_rows}\\n\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_minor_test))\n",
    "print(f\"Total number of minor test set rows: {formatted_num_rows}\\n\")\n",
    "\n",
    "'''\n",
    "JSON_10 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/major_train_processed.json\"\n",
    "JSON_11 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/major_test_processed.json\"\n",
    "\n",
    "df_major_train = pd.read_json(JSON_10, lines = \"True\")\n",
    "df_major_test = pd.read_json(JSON_11, lines = \"True\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_major_train))\n",
    "print(f\"Total number of major train set rows: {formatted_num_rows}\\n\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_major_test))\n",
    "print(f\"Total number of major test set rows: {formatted_num_rows}\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_id', 'version', 'yymm', 'created', 'title', 'secondary_subfield',\n",
       "       'abstract', 'primary_subfield', 'field', 'fulltext',\n",
       "       'processed_title_abstract_fulltext', 'processed_title_abstract'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minor_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discontinuous Euler instability in nanoelectromechanical systems'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minor_train['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minor_train['processed_title_abstract_fulltext'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDiscontinuous Euler instability in nanoelectromechanical systems\\n\\nGuillaume Weick,1, 2 Fabio Pistolesi,3, 4 Eros Mariani,1, 5 and Felix von Oppen1\\n\\n1Dahlem Center for Complex Quantum Systems and Fachbereich Physik,\\n\\nFreie Universitat Berlin, D-14195 Berlin, Germany\\n\\n2IPCMS (UMR 7504), CNRS and Universit´e de Strasbourg, F-67034 Strasbourg, France\\n3CPMOH (UMR 5798), CNRS and Universit´e de Bordeaux I, F-33405 Talence, France\\n4LPMMC (UMR 5493), CNRS and Universit´e Joseph Fourier, F-38042 Grenoble, France\\n\\n5School of Physics, University of Exeter, Stocker Road, Exeter, EX4 4QL, UK\\n\\n(Dated: November 1, 2018)\\n\\nWe investigate nanoelectromechanical systems near mechanical instabilities. We show that quite\\ngenerally, the interaction between the electronic and the vibronic degrees of freedom can be ac-\\ncounted for essentially exactly when the instability is continuous. We apply our general framework\\nto the Euler buckling instability and find that the interaction between electronic and vibronic de-\\ngrees of freedom qualitatively affects the mechanical instability, turning it into a discontinuous one\\nin close analogy with tricritical points in the Landau theory of phase transitions.\\n\\nPACS numbers: 73.63.-b, 85.85.+j, 63.22.Gh\\n\\nI.\\n\\nINTRODUTION\\n\\nThe buckling of an elastic rod by a longitudinal com-\\npression force F applied to its two ends constitutes the\\nparadigm of a mechanical\\ninstability, called buckling\\ninstability.1 It was first studied by Euler in 1744 while in-\\nvestigating the maximal load that a column can sustain.2\\nAs long as F stays below a critical force Fc, the rod re-\\nmains straight, while for F > Fc it buckles, as sketched in\\nFig. 1a-b. The transition between the two states is con-\\ntinuous and the frequency of the fundamental bending\\nmode vanishes at the instability.\\n\\nThere has been much recent interest in exploring buck-\\nling instabilities in nanomechanical systems.\\nIn the\\nquest to understand the remarkable mechanical prop-\\nerties of nanotubes,3 -- 5 there have been observations of\\ncompressive buckling instabilities in this system.6 The\\nEuler buckling instability has been observed in SiO2\\nnanobeams and shown to obey continuum elasticity\\ntheory.7 There are also close relations with the recently\\nobserved wrinkling8 and possibly with the rippling9 of\\nsuspended graphene samples. Theoretical works have\\nstudied the quantum properties of nanobeams near the\\nEuler instability,10 -- 13 proposing this system to explore\\nzero-point fluctuations of a mechanical mode11 or to serve\\nas a mechanical qubit.13\\n\\nIn this work, we study the interaction of current flow\\nwith the vibrational motion near such continuous me-\\nchanical\\ninstabilities which constitutes a fundamental\\nissue of nanoelectromechanics.14 Remarkably, we find\\nthat under quite general conditions, this problem ad-\\nmits an essentially exact solution due to the continu-\\nity of the instability and the consequent vanishing of\\nthe vibronic frequency at the transition (\"critical slow-\\ning down\").\\nIn fact, the vanishing of the frequency\\nimplies that the mechanical motion becomes slow com-\\npared to the electronic dynamics and an appropriate non-\\nequilibrium Born-Oppenheimer (NEBO) approximation\\nbecomes asymptotically exact near the transition. Here,\\n\\nFIG. 1: (color online) Sketch of a nanobeam (a) in the flat\\nstate and (b) the buckled state with two equivalent metastable\\npositions of the rod (solid and dashed lines). An equivalent\\ncircuit of the embedded SET is shown in (c).\\n\\nwe illustrate our general framework by applying it to the\\nnanoelectromechanics of the Euler instability.\\n\\nWe find that the interplay of electronic transport and\\nthe mechanical instability causes significant qualitative\\nchanges both in the nature of the buckling and in the\\ntransport properties.\\nIn leading order, the NEBO ap-\\nproximation yields a current-induced conservative force\\nacting on the vibronic mode. At this order, our principal\\nconclusion is that the coupling to the electronic dynamics\\ncan change the nature of the buckling instability from a\\ncontinuous to a discontinuous transition which is closely\\nanalogous to tricritical behavior in the Landau theory\\nof phase transitions. Including in addition the fluctua-\\ntions of the current-induced force as well as the corre-\\nsponding dissipation leads to Langevin dynamics of the\\nvibrational mode which becomes important in the vicin-\\nity of the discontinuous transition. Employing the same\\nNEBO limit to deduce the electronic current, we find that\\nthe buckling instability induces a current blockade over\\na wide range of parameters. This is a manifestation of\\nthe Franck-Condon blockade15 -- 17 whenever the buckling\\ninstability remains continuous but is caused by a novel\\ntricritical blockade when the instability is discontinuous.\\nThe emergence of a current blockade in the buckled state\\nsuggests that our setup could, in principle, serve as a\\nmechanically-controlled switching device.\\n\\n\\x0cII. MODEL\\n\\nClose to the Euler instability, the frequency of the fun-\\ndamental bending mode of the beam approaches zero,\\nwhile all higher modes have a finite frequency.1 This al-\\nlows us to retain only the fundamental mode of amplitude\\nX (see Fig. 1a-b) and following previous studies,10 -- 12 we\\nreduce the vibrational Hamiltonian to18\\n\\nHvib =\\n\\nP 2\\n2m\\n\\n+\\n\\nmω2\\n\\n2\\n\\nX 2 +\\n\\nα\\n4\\n\\nX 4,\\n\\n(1)\\n\\nwhich is closely analogous to the Landau theory of contin-\\nuous phase transitions. In Eq. (1), P is the momentum\\nconjugate to X and m denotes an effective mass. The\\nmode frequency ω2 ∼ 1 − F/Fc changes sign when F\\nreaches a critical force Fc. Global stability then requires\\na quartic term with α > 0. Thus, for F < Fc (ω2 > 0),\\nX = 0 is the only stable minimum and the beam remains\\nstraight. For F > Fc (ω2 < 0), the beam buckles into\\n\\none of the two minima at ±X+ = ±(cid:112)−mω2/α.\\n\\nThe vibronic mode of the nanobeam interacts with an\\nembedded metallic single-electron transistor (SET), con-\\nsisting of a small metallic island coupled to source, drain,\\nand gate electrode (Fig. 1c). We assume that the SET\\noperates in the Coulomb blockade regime19 and that bias\\nand gate voltage are tuned to the vicinity of the conduct-\\ning region between SET states with, say, zero and one\\nexcess electron. The electronic degrees of freedom cou-\\nple to the vibronic motion through the occupation n of\\nexcess electrons on the metallic island. Specifically, we\\nassume that the electron-vibron coupling does not break\\nthe underlying parity symmetry of the vibronic dynam-\\nics under X → −X. This follows naturally when the\\ncoupling emerges from the electron-phonon coupling in-\\ntrinsic to the nanobeam20 and implies that the coupling\\ndepends only on even powers of the vibronic mode coor-\\ndinate X. The dominant coupling is quadratic in X,\\n\\nwith a coupling constant g > 0.20 When there is a signif-\\nicant contribution to the electron-vibron coupling origi-\\nnating from the electrostatic dot-gate interaction, we en-\\nvision a symmetric gate setup consistent with Eq. (2).\\n\\nIn the presence of the vibronic dynamics X(t), the elec-\\ntronic occupation n(X, t) of the island is described by the\\nBoltzmann-Langevin equation21\\n\\n= {n, Hvib} + Γ+(1 − n) − Γ−n + δJ+ − δJ−.\\n\\n(3)\\n\\ndn\\ndt\\n\\nThis equation assumes that the bias is large compared\\nto temperature so that tunneling is effectively unidi-\\nrectional and the relevant tunneling rates Γ± for tun-\\nneling onto (+) and off (−) the island are given by\\nΓ± = R−1(V /2 ± ¯Vg)Θ(V /2 ± ¯Vg). Here, R denotes\\nthe tunneling resistances (R (cid:29) h/e2) between island and\\nleads, V is the bias voltage, Θ(x) denotes the Heaviside\\n\\n2\\n\\nstep function, and we set \\x02 = e = 1. Since both gate\\nvoltage (via the capacitances C and Cg in Fig. 1c) and vi-\\nbronic deformations couple to the excess charge n on the\\nisland, the effective gate voltage ¯Vg = Vg − gX 2/2 com-\\nbines the gate potential Vg (measured from the degen-\\neracy point between the states with zero and one excess\\nelectron) and the vibron-induced shift of the electronic\\nenergy described by Hc. The stochastic Poisson nature\\nof electronic tunneling is accounted for by including the\\nLangevin sources δJ± with correlators (cid:104)δJ+(t)δJ+(t(cid:48))(cid:105) =\\nΓ+(1 − n)δ(t − t(cid:48)) and (cid:104)δJ−(t)δJ−(t(cid:48))(cid:105) = Γ−nδ(t − t(cid:48)).\\nThe vibronic dynamics enters Eq. (3) through the Pois-\\nson bracket {n, Hvib}.\\n\\nIII. STABILITY ANALYSIS\\n\\nWe are now in a position to investigate the influence\\nof the electronic dynamics on the vibronic motion. Near\\nthe instability, the vibrational dynamics becomes slow\\ncompared to the electronic tunneling dynamics. As has\\nrecently been shown,22,23 the effect of the current on\\nthe vibrational motion can then be described within a\\nNEBO approximation in which the vibrational motion is\\nsubject to a current-induced force −gXn(X, t) originat-\\ning in the electron-vibron interaction (2). This current-\\ninduced force involves both a time-averaged and conser-\\nvative force as well as fluctuating and frictional forces,\\nresulting in Langevin dynamics of the vibronic degree of\\nfreedom.\\n\\nIn lowest order, the Langevin dynamics only involves\\nthe conservative force which emerges from the average\\noccupation n0(X) in the absence of fluctuations (δJ± (cid:39)\\n0) and vibronic dynamics ({n, Hvib} (cid:39) 0). In this limit,\\nEq. (3) reduces to the usual rate equation of a metallic\\nSET so that19\\n\\n1,\\n1\\n2\\n0,\\n\\nvg(x) > v/2,\\n\\n+\\n\\nvg(x)\\n\\nv\\n\\n, −v/2 (cid:54) vg(x) (cid:54) v/2,\\n\\nvg(x) < −v/2\\n\\n(4)\\n\\nwith v > 0 and vg(x) = vg − x2/2. Here and below,\\nwe employ dimensionless variables by introducing char-\\n\\nacteristic scales E0 = g2/α of energy, l0 = (cid:112)g/α of\\nlength, and ω0 = (cid:112)g/m of frequency (or time t) from\\n\\na comparison of the quartic vibron potential in Hvib and\\nthe electron-vibron coupling Hc. Specifically, we intro-\\nduce the reduced variables x = X/l0, p = P/mω0l0,\\nτ = ω0t, v = V /E0, vg = Vg/E0, and r = Rω0/E0. In\\nterms of these variables, we can also write Hvib + Hc =\\nE0[p2/2 + (−\\x01 + n)x2/2 + x4/4] in terms of a reduced\\ncompressional force \\x01 = −mω2/g.\\nThe current-induced force −xn0(x) has dramatic ef-\\nfects on the Euler instability, as follows from a stability\\nanalysis of the vibrational motion. The (meta)stable po-\\nsitions of the nanobeam are obtained by setting the effec-\\ntive force feff (x) = \\x01x − x3 − xn0(x) to zero. Our results\\n\\nHc =\\n\\ng\\n2\\n\\nX 2 n,\\n\\n(2)\\n\\nn0(x) =\\n\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n\\n\\x0c3\\n\\nFIG. 2: (color online) (Meta)stable (solid blue lines) and unstable (dashed red lines) positions of the nanobeam vs. scaled force\\n\\x01 for (a) vg < v/2, v < 1/2, (b) vg < v/2, v > 1/2, (c) vg > v/2, v < 1/2 (for \\x01+ > 1; a similar plot holds for \\x01+ < 1), (d)\\nvg > v/2, v > 1/2, as indicated in the vg -- v plane in (e). The dotted blue line is the result without electron-vibron coupling.\\n+ = \\x01 − 1, and x2− = (\\x01 − \\x01)/(1 − 1/2v). Grey indicates conducting regions.\\nNotation: \\x01± = 2vg ± v, \\x01 = 1/2 + vg/v, x2\\n\\n+ = \\x01, x2\\n\\nare summarized in the stability diagrams in Fig. 2. The\\nmost striking results of this analysis are: (i) The current\\nflow renormalizes the critical force required for buckling\\ntowards larger values.\\n(ii) At low biases, the buckled\\nstate can appear via a discontinuous transition.\\n\\nThese results can be understood most directly in terms\\nof the potential veff (x) associated with feff (x). Focusing\\non the current-carrying region (shown in grey in Fig. 2\\nand delineated by max{0, \\x01−} < x2 < \\x01+ with \\x01± =\\n2vg ± v), we find\\n\\nveff (x) =\\n\\n1\\n2\\n\\n−\\x01 +\\n\\nv + 2vg\\n\\n2v\\n\\nx2 +\\n\\n1\\n4\\n\\n1 − 1\\n2v\\n\\nx4.\\n\\n(5)\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nThe quadratic term shows that the current indeed stabi-\\nlizes the unbuckled state, renormalizing the critical force\\nto \\x01 = 1/2 + vg/v when \\x01− < 0 < \\x01+ (Fig. 2a-b). Re-\\nmarkably, however, the current-induced contribution to\\nthe quartic term is negative at small x2 and thus desta-\\nbilizes the unbuckled state. According to Eq. (5), the\\nquartic term in the current-induced potential becomes in-\\ncreasingly significant as the bias voltage v decreases and\\nwe find that the overall prefactor of the quartic term be-\\ncomes negative when v < 1/2.24 It is important to note\\nthat this does not imply a globally unstable potential\\nsince the current-induced force contributes only for small\\nx2. A sign reversal of the quartic term is also a famil-\\niar occurrence in the Landau theory of tricritical points\\nwhich connect between second- and first-order transition\\nlines.25 In close analogy, the sign reversal of the quartic\\nterm in the effective potential (5) signals a discontinuous\\nEuler instability which reverts to a continuous transition\\nat biases v > 1/2 where the prefactor of the quartic term\\nremains positive.\\n\\nSpecifically, when v > 1/2 (Fig. 2b,d), the current-\\ninduced potential renormalizes the parameters of the vi-\\nbronic Hamiltonian but leaves the quartic term posi-\\ntive. This modifies how the position of the minimum\\ndepends on the applied force in the conducting region\\nmax{0, \\x01−} < x2 < \\x01+, but the Euler instability remains\\ncontinuous. When v < 1/2, the equilibrium position\\nat finite x becomes unstable within the entire current-\\ncarrying region. This leads to a discontinuous Euler tran-\\nsition when \\x01− < 0 < \\x01+ (Fig. 2a) and to multistability\\n\\nin the region \\x01− < x2 < \\x01+ when \\x01− > 0 (Fig. 2c).26\\n\\nAt the level of the stability analysis, we can also ob-\\ntain the current I by evaluating the rate-equation result19\\nRI(x)/V = 1/4 − [vg(x)/v]2 at the position of the most\\nstable minimum. Corresponding results in the vg -- v\\nplane are shown in Fig. 3a-f for various values of the\\napplied force \\x01. By comparison with the Coulomb di-\\namond in the absence of the electron-vibron coupling\\n(dotted lines in Fig. 3), we see that the Euler instability\\nleads to a current blockade over a significant parameter\\nrange. For v > 1/2, the blockade is a manifestation of\\nthe Franck-Condon blockade,15,17 caused by the induced\\nlinear electron-vibron coupling when expanding Eq. (2)\\nabout the buckled state.\\n\\nIn contrast, for v < 1/2, the current blockade is a\\ndirect consequence of the discontinuous Euler instabil-\\n\\nFIG. 3: (color online) Conductance G = RI/V in the vg -- v\\nplane for applied force (a) \\x01 (cid:54) 0, (b) \\x01 = 0.25, (c,g) \\x01 = 0.5,\\n(d) \\x01 = 0.75, (e) \\x01 = 1, (f,h) \\x01 = 1.25, within (a-f) stability\\nanalysis and (g-h) full Langevin dynamics (r = γe = T =\\n0.01). Color scale: G = 0 → 1/4 from dark blue to white.\\nDotted lines delineate the Coulomb diamond for g = 0.\\n\\n\\x0c4\\n\\nity. We have seen above that in this regime, the buckled\\nstate becomes unstable throughout the entire current-\\ncarrying region. As a result, the current-induced force\\nwill always drive the system out of the current-carrying\\nregion, explaining the current blockade. An intriguing\\nfeature of this novel tricritical current blockade is the\\ncurved boundary of the apparent Coulomb-blockade dia-\\nmond (Fig. 3), a behavior which is actually observed in\\nnanoelectromechanical systems.\\n\\nPlanck equation (6). Numerical results for the scaled lin-\\near conductance G = RI/V are shown in Fig. 3g-h, using\\nthe same parameters as in Fig. 3c,f. We observe that the\\nfluctuations reduce the size of the blockaded region and\\nblur the edges of the conducting regions as the system\\ncan explore more conducting states in phase space. Nev-\\nertheless, the conclusions of the stability analysis clearly\\nremain valid qualitatively.\\n\\nIV. LANGEVIN DYNAMICS\\n\\nV. CONCLUSION\\n\\nTo investigate the robustness of the stability analysis\\nagainst fluctuations, we turn to the complete vibronic\\nLangevin dynamics x + γ(x) x = feff(x) + ξ(τ ). The\\nfluctuating force ξ(τ ) is generated by fluctuations of the\\nelectronic occupation and the frictional force −γ(x) x by\\nthe delayed response of the electrons to the vibronic\\ndynamics. To compute γ(x) and ξ(τ ), we solve Eq.\\n(3) including the vibronic dynamics and the Langevin\\nsources. Writing separate equations for average and\\nfluctuations of the occupation by setting n = ¯n + δn,\\nwe see that the leading correction to ¯n arises from the\\nPoisson bracket, yielding ¯n = n0 −\\nX∂X n0. At\\nthe same time, the fluctuations δn obey the correla-\\ntor (cid:104)δn(t)δn(t(cid:48))(cid:105) =\\nInsert-\\ning these results into the expression for the current-\\ninduced force −gXn and employing reduced units, we\\nfind (cid:104)ξ(τ )ξ(τ(cid:48))(cid:105) = D(x)δ(τ−τ(cid:48)) with diffusion and damp-\\ning coefficients D(x) = 2rx2n0(1 − n0)/v and γ(x) =\\n−rx∂xn0/v, respectively. Finally, we can pass from the\\nLangevin to the equivalent Fokker-Planck equation22,23\\nfor the probability P(x, p, τ ) that the nanobeam is at po-\\nsition x and momentum p at time τ ,\\n\\nΓ++Γ− n0(1 − n0)δ(t − t(cid:48)).\\n\\n2\\n\\n1\\n\\nΓ++Γ−\\n\\nWe have presented a general approach to the interplay\\nbetween continuous mechanical instabilities and current\\nflow in nanoelectromechanical systems, and have applied\\nour general framework to the Euler buckling instabil-\\nity. The current flow modifies the nature of the buck-\\nling instability from a continuous to a tricritical transi-\\ntion. Likewise, the instability induces a novel tricritical\\ncurrent blockade at low bias. Our nonequilibrium Born-\\nOppenheimer approach generalizes not only to other con-\\ntinuous mechanical instabilities, but also to other systems\\nsuch as semiconductor quantum dots or single-molecule\\njunctions with a discrete electronic spectrum, to other\\ntypes of electron-vibron coupling,28 and to further trans-\\nport characteristics (e.g., current noise).\\n\\nOur proposed setup can be realized experimentally by\\nclamping, e.g., a suspended carbon nanotube and apply-\\ning a force to atomic precision either using a break junc-\\ntion or an atomic force microscope. Indeed, several recent\\nexperiments show that the electron-vibron coupling is\\nsurprisingly strong in suspended carbon nanotube quan-\\ntum dots.4,5,16\\n\\n∂τP = −p∂xP − feff∂pP + γ∂p(pP) +\\n\\npP.\\n∂2\\n\\n(6)\\n\\nAcknowledgments\\n\\nD\\n2\\n\\nThe current I =(cid:82) dxdpPst(x, p)I(x) is now obtained\\n\\nNote that the diffusion and damping coefficients are non-\\nvanishing only in the conducting region.27\\nfrom the stationary solution ∂τPst = 0 of the Fokker-\\n\\nWe acknowledge financial support through Sfb 658 of\\nthe DFG (GW, EM, FvO) and ANR contract JCJC-\\n036 NEMESIS (FP). FvO enjoyed the hospitality of the\\nKITP (NSF PHY05-51164).\\n\\n1 L. D. Landau and E. M. Lifshitz, Theory of Elasticity\\n\\n(Pergamon Press, Oxford, 1970).\\n\\n2 L. Euler, in Leonhard Euler\\'s Elastic Curves, translated\\nand annotated by W. A. Oldfather, C. A. Ellis, and D. M.\\nBrown, reprinted from ISIS, No. 58 XX(1), 1744 (Saint\\nCatherine Press, Bruges).\\n\\n3 P. Poncharal et al., Science 283, 1513 (1999).\\n4 G. A. Steele et al., Science 325, 1103 (2009).\\n5 B. Lassagne et al., Science 325, 1107 (2009).\\n6 M. R. Falvo et al., Nature 389, 582 (1997).\\n7 S. M. Carr and M. N. Wybourne, Appl. Phys. Lett. 82,\\n\\n709 (2003).\\n\\n9 J. C. Meyer et al., Nature 446, 60 (2007).\\n10 S. M. Carr, W. E. Lawrence, and M. N. Wybourne, Phys.\\n\\nRev. B 64, 220101(R) (2001).\\n\\n11 P. Werner and W. Zwerger, Europhys. Lett. 65, 158 (2004).\\n12 V. Peano and M. Thorwart, New J. Phys. 8, 21 (2006).\\n13 S. Savel\\'ev, X. Hu, and F. Nori, New J. Phys. 8, 105 (2006).\\n14 H. G. Craighead, Science 290, 1532 (2000); M. L. Roukes,\\n\\nPhys. World 14, 25 (2001).\\n\\n15 J. Koch and F. von Oppen, Phys. Rev. Lett. 94, 206804\\n\\n(2005).\\n\\n16 R. Leturcq et al., Nature Phys. 5, 327 (2009).\\n17 F. Pistolesi and S. Labarthe, Phys. Rev. B 76, 165317\\n\\n8 W. Bao et al., Nature Nanotech. 4, 562 (2009).\\n\\n(2007).\\n\\n\\x0c18 The rotation of the plane of the buckled nanobeam is as-\\n\\nsumed massive due to clamped boundary conditions.\\n\\n19 See, e.g., Chap. 3 in T. Dittrich et al., Quantum Transport\\n\\nand Dissipation (Wiley-VCH, Weinheim, 1998).\\n\\n20 E. Mariani and F. von Oppen, Phys. Rev. B 80, 155411\\n\\n(2009).\\n\\n21 See Ya. M. Blanter and M. Buttiker, Phys. Rep. 336, 1\\n\\n(2000) for a review of the Boltzmann-Langevin method.\\n\\n22 Ya. M. Blanter, O. Usmani, and Yu. V. Nazarov, Phys.\\nRev. Lett. 93, 136802 (2004); ibid. 94, 049904(E) (2005).\\n23 D. Mozyrsky, M. B. Hastings, and I. Martin, Phys. Rev. B\\n\\n73, 035104 (2006).\\n\\n24 We note that the singularity at small v is cut off for bias\\nvoltages of the order of temperature or level broadening.\\n\\n5\\n\\n25 See, e.g., P. M. Chaikin and T. C. Lubensky, Principles of\\nCondensed Matter Physics (Cambridge University Press,\\nCambridge, 1995).\\n\\n26 In a quite different context, a discontinuous Euler insta-\\nbility has also been predicted in: S. Savel\\'ev and F. Nori,\\nPhys. Rev. B 70, 214415 (2004).\\n\\n27 In some cases, a stable numerical solution of Eq. (6) re-\\n\\nquires a small extrinsic damping γe and temperature T .\\n\\n28 E.g., a small symmetry-breaking coupling linear in X leads\\nto a tricritical point in an external field, a purely linear\\ncoupling to a 2nd order transition in a field, G. Weick et\\nal., unpublished.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minor_train['fulltext'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting training embeddings from doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 14:57:30,154 : INFO : loading Doc2Vec object from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6\n",
      "2024-08-20 14:57:30,975 : INFO : loading dv recursively from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.dv.* with mmap=None\n",
      "2024-08-20 14:57:30,976 : INFO : loading vectors from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.dv.vectors.npy with mmap=None\n",
      "2024-08-20 14:57:30,998 : INFO : loading wv recursively from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.wv.* with mmap=None\n",
      "2024-08-20 14:57:30,999 : INFO : loading vectors from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.wv.vectors.npy with mmap=None\n",
      "2024-08-20 14:57:31,649 : INFO : loading syn1neg from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.syn1neg.npy with mmap=None\n",
      "2024-08-20 14:57:32,264 : INFO : setting ignored attribute cum_table to None\n",
      "2024-08-20 14:57:41,499 : INFO : Doc2Vec lifecycle event {'fname': '/Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6', 'datetime': '2024-08-20T14:57:41.499426', 'gensim': '4.3.2', 'python': '3.11.0 (main, Jun 20 2024, 12:11:23) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.5-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#enable logging to monitor training loss\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#load doc2vec model that you have trained using the doc2vec_training.py script \n",
    "DOC2VEC_model = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6\"\n",
    "\n",
    "loaded_model = Doc2Vec.load(DOC2VEC_model)\n",
    "# df_v188_subset_90_tokens = pd.read_json(processed_path, lines = True, dtype={'id': str})\n",
    "#processed_title_authors_abstracts = [entry.spilt() for entry in df_minor_train['processed_title_authors_abstract']]\n",
    "\n",
    "#data = []\n",
    "#abstracts = [sentence.split() for sentence in df_major_train['abstract']]\n",
    "#titles = [sentence.split() for sentence in df_major_train['title']]\n",
    "#unique_id = [id for id in df_major_train['paper_id']]\n",
    "#categories = [sentence for sentence in df_major_train['primary_subfield']]\n",
    "#primary_categories = [category.split()[0] for category in categories]\n",
    "#for title, abstract in zip(titles, abstracts):\n",
    "#    combined = title + abstract\n",
    "#    data.append(combined)\n",
    "\n",
    "inferred_vectors = []\n",
    "data = df_minor_train[\"processed_title_abstract\"]\n",
    "\n",
    "def infer_vector(doc, steps=100, alpha=0.001): \n",
    "    return loaded_model.infer_vector(doc)\n",
    "#use ThreadPoolExecutor to parallelize the inference\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(infer_vector, doc, steps=100, alpha=0.001) for doc in data]\n",
    "    inferred_vectors = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "#normalize the inferred vectors\n",
    "scaler = StandardScaler()\n",
    "normalized_inferred_vectors = scaler.fit_transform(inferred_vectors)\n",
    "\n",
    "print(\"Doc2vec model successfully inferred: \")\n",
    "normalized_inferred_vectors_np = np.array(normalized_inferred_vectors)\n",
    "\n",
    "#save inferred_vectors_np to .npy file\n",
    "output_name = f'Performance_Metrics/datasets/X_train.npy'\n",
    "np.save(output_name, normalized_inferred_vectors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "categories = df_minor_train['primary_subfield']\n",
    "primary_categories = [category.split()[0] for category in categories]\n",
    "y_train = primary_categories\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_integer = label_encoder.transform(y_train)\n",
    "num_categories = len(label_encoder.classes_)\n",
    "#print(label_encoder.classes_)\n",
    "y_train_one_hot = to_categorical(y_integer, num_classes = num_categories)\n",
    "y_train_one_hot_np = np.array(y_train_one_hot) \n",
    "np.save(f'/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_train.npy', y_train_one_hot_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting testing embeddings from doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:16:23,618 : INFO : loading Doc2Vec object from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6\n",
      "2024-07-28 21:16:24,311 : INFO : loading dv recursively from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.dv.* with mmap=None\n",
      "2024-07-28 21:16:24,312 : INFO : loading vectors from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.dv.vectors.npy with mmap=None\n",
      "2024-07-28 21:16:24,328 : INFO : loading wv recursively from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.wv.* with mmap=None\n",
      "2024-07-28 21:16:24,329 : INFO : loading vectors from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.wv.vectors.npy with mmap=None\n",
      "2024-07-28 21:16:24,825 : INFO : loading syn1neg from /Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6.syn1neg.npy with mmap=None\n",
      "2024-07-28 21:16:25,326 : INFO : setting ignored attribute cum_table to None\n",
      "2024-07-28 21:16:34,676 : INFO : Doc2Vec lifecycle event {'fname': '/Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6', 'datetime': '2024-07-28T21:16:34.676830', 'gensim': '4.3.2', 'python': '3.11.0 (main, Jun 20 2024, 12:11:23) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.5-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2vec model successfully inferred: \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "DOC2VEC_model = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/doc2vec_model_Cornell_minor_100_3_6\"\n",
    "\n",
    "loaded_model = Doc2Vec.load(DOC2VEC_model)\n",
    "\n",
    "#data = []\n",
    "#abstracts = [sentence.split() for sentence in df_major_test['abstract']]\n",
    "#titles = [sentence.split() for sentence in df_major_test['title']]\n",
    "# Combine/concatenate the lists of words\n",
    "#for title, abstract in zip(titles, abstracts):\n",
    "#    combined = title + abstract\n",
    "#    data.append(combined)\n",
    "\n",
    "inferred_vectors = []\n",
    "data = df_minor_test[\"processed_title_abstract\"]\n",
    "\n",
    "# steps=50, alpha=0.01\n",
    "def infer_vector(doc, steps=100, alpha=0.001):\n",
    "    return loaded_model.infer_vector(doc)\n",
    "# Use ThreadPoolExecutor to parallelize the inference\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(infer_vector, doc, steps=100, alpha=0.001) for doc in data]\n",
    "    inferred_vectors = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "# Normalize the test inferred vectors\n",
    "# scaler has been fit for training inferred vector, so we just transform the test inferred vectors to ensure consistency \n",
    "normalized_inferred_vectors = scaler.transform(inferred_vectors)\n",
    "\n",
    "print(\"Doc2vec model successfully inferred: \")\n",
    "normalized_inferred_vectors_np = np.array(normalized_inferred_vectors)\n",
    "\n",
    "# Save inferred_vectors_np to .npy file\n",
    "output_name = f'Performance_Metrics/datasets/X_test.npy'\n",
    "np.save(output_name, normalized_inferred_vectors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoriess = [sentence for sentence in df_minor_test['primary_subfield']]\n",
    "y_test = [category.split()[0] for category in categoriess]\n",
    "y_test_integer = label_encoder.transform(y_test)\n",
    "\n",
    "num_training_categories = len(set(y_train))\n",
    "y_test_one_hot = to_categorical(y_test_integer, num_classes = num_training_categories)\n",
    "y_test_one_hot_np = np.array(y_test_one_hot) \n",
    "np.save(f'/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_test.npy', y_test_one_hot_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON_extracted_vectors = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/X.npy\"\n",
    "import matplotlib.pyplot as plt\n",
    "VECTOR_SIZE = 300\n",
    "num_training_categories = len(label_encoder.classes_)\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "    # Load datasets\n",
    "    X_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_test.npy')\n",
    "    y_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_test.npy')\n",
    "    X_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_train.npy')\n",
    "    y_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_train.npy')\n",
    "\n",
    "    num_neurons_configs = [450, 500, 550, 600, 650, 700, 750, 770, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300]\n",
    "    #num_neurons = 800\n",
    "    # 800 is best for major \n",
    "    #dropout_rates = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n",
    "    dropout_rate = 0.64\n",
    "\n",
    "    out_of_sample_accuracies = []\n",
    "    overfitting_ratios = []\n",
    "\n",
    "    with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\", 'a') as file:\n",
    "        file.write(\"\\n════════════════════════════════════\\n\")\n",
    "        file.write(\"ANN's performance metrics with 0.001 LR, 2048 batch_size, 30 epochs, 2 hidden layers with various num of neurons and {dropout_rate} dropout\\n\")\n",
    "        file.write(\"ANN is trained on embeddings of {VECTOR_SIZE} vector size, inferred with step=100, alpha=0.001, using titles + abstracts from Cornell huggingface major dataset. \\n\")\n",
    "        file.write(\"Doc2vec is trained on titles + abstracts + fulltexts from major dataset with epoch = 100, alpha=0.02, min_alpha=0.0001 \\n\")\n",
    "        \n",
    "        for num_neurons in num_neurons_configs:\n",
    "            file.write(f\"\\nConfiguration: {num_neurons} neurons in each hidden layer\\n\")\n",
    "            \n",
    "            # Define regularization parameters\n",
    "            l2_regularization_factor = 0.01\n",
    "\n",
    "            # Build the model\n",
    "            arXiv_classification_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(VECTOR_SIZE,)),\n",
    "                #tf.keras.layers.Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_regularization_factor)),\n",
    "                #tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_neurons, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_neurons, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_training_categories, activation='softmax')\n",
    "            ])\n",
    "            # Compile the model\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "            # Compile the model & TensorFlow with Metal automatically distribute operations across available GPUs      \n",
    "            arXiv_classification_model.compile(optimizer=optimizer,\n",
    "                        loss='categorical_crossentropy',  # a multi-category classification task expects the targets to be in one-hot encoded format\n",
    "                        metrics=[\n",
    "                            'accuracy',                                      # Basic accuracy\n",
    "                            tf.keras.metrics.Precision(name='precision'),    # Precision metric\n",
    "                            tf.keras.metrics.Recall(name='recall'),          # Recall metric\n",
    "                            CosineSimilarity(name='cosine_similarity')       # Cosine Similarity    \n",
    "                        ])\n",
    "            \n",
    "            # Fitting an ANN model\n",
    "            History = arXiv_classification_model.fit(X_train, y_train, batch_size=2048, epochs=20, verbose=1)\n",
    "            # batch_size = 512 seems to be \"optimal\" for training speed while using gpus with METAL support on Mac\n",
    "            # h5 format is fine, but we should use the native Keras format for saving models\n",
    "            arXiv_classification_model.save(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/v188_ANN_model.keras\")\n",
    "            # In-sample performance metrics\n",
    "            file.write(\"\\nIn sample performance metrics: \\n\")\n",
    "            in_sample_metrics = arXiv_classification_model.evaluate(X_train, y_train, verbose=0)\n",
    "            file.write(f\"Loss: {in_sample_metrics[0]}, Accuracy: {in_sample_metrics[1]}, Precision: {in_sample_metrics[2]}, Recall: {in_sample_metrics[3]}, Cosine Similarity: {in_sample_metrics[4]}\" + \"\\n\")\n",
    "            # Out-of-sample performance metrics\n",
    "            file.write(\"\\nOut of sample performance metrics: \\n\")\n",
    "            out_of_sample_metrics = arXiv_classification_model.evaluate(X_test, y_test, verbose=0)\n",
    "            file.write(f\"Loss: {out_of_sample_metrics[0]}, Accuracy: {out_of_sample_metrics[1]}, Precision: {out_of_sample_metrics[2]}, Recall: {out_of_sample_metrics[3]}, Cosine Similarity: {out_of_sample_metrics[4]}\" + \"\\n\")\n",
    "            #store the out-of-sample accuracy\n",
    "            out_of_sample_accuracies.append(out_of_sample_metrics[1])\n",
    "            # Calculate overfitting ratio = test loss / train loss\n",
    "            overfitting_ratio = out_of_sample_metrics[0] / in_sample_metrics[0]\n",
    "            overfitting_ratios.append(overfitting_ratio)\n",
    "            acceptable_threshold = 1.5\n",
    "            overfitting_status = \"acceptable\" if overfitting_ratio <= acceptable_threshold else \"above threshold\"\n",
    "            file.write(f\"Overfitting Ratio: {overfitting_ratio:.2f}\\n\")\n",
    "            file.write(f\"Overfitting Status: {overfitting_status}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# Write the elapsed time and timestamp to the file\n",
    "with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\", 'a') as file:\n",
    "    file.write(f\"Elapsed time: {elapsed_time / 60} minutes \\n\")\n",
    "    file.write(f\"Timestamp: {current_time}\\n\")\n",
    "# Print the elapsed time and timestamp\n",
    "print(f\"Elapsed time: {elapsed_time / 60} minutes\")\n",
    "print(f\"Timestamp: {current_time} \\n\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Out-of-Sample Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_neurons_configs, out_of_sample_accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Out-of-Sample Accuracy')\n",
    "plt.title('Out-of-Sample Accuracy vs. num_neurons')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Overfitting Ratio\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_neurons_configs, overfitting_ratios, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Overfitting Ratio')\n",
    "plt.title('Overfitting Ratio vs. num_neurons')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON_extracted_vectors = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/X.npy\"\n",
    "import matplotlib.pyplot as plt\n",
    "VECTOR_SIZE = 300\n",
    "num_training_categories = len(label_encoder.classes_)\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "    # Load datasets\n",
    "    X_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_test.npy')\n",
    "    y_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_test.npy')\n",
    "    X_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_train.npy')\n",
    "    y_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_train.npy')\n",
    "\n",
    "    #num_neurons_configs = [600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1500]\n",
    "    num_neurons = 1000\n",
    "    # 800 is best for major \n",
    "    dropout_rates = [0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68]\n",
    "    dropout_rate = 0.64\n",
    "    #learning_rates = [0.0001, 0.0005, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007]\n",
    "    learning_rate = 0.001\n",
    "    out_of_sample_accuracies = []\n",
    "    overfitting_ratios = []\n",
    "\n",
    "    with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\", 'a') as file:\n",
    "        file.write(\"\\n════════════════════════════════════\\n\")\n",
    "        file.write(\"ANN's performance metrics with 0.001 LR, 2048 batch_size, 20 epochs, 2 hidden layers with various num of neurons and {dropout_rate} dropout\\n\")\n",
    "        file.write(\"ANN is trained on embeddings of {VECTOR_SIZE} vector size, inferred with step=100, alpha=0.001, using titles + abstracts from Cornell huggingface major dataset. \\n\")\n",
    "        file.write(\"Doc2vec is trained on titles + abstracts + fulltexts from major dataset with epoch = 80, alpha=0.03, min_alpha=0.0001 \\n\")\n",
    "        \n",
    "        for dropout_rate in dropout_rates:\n",
    "            file.write(f\"\\nConfiguration: {num_neurons} neurons in each hidden layer\\n\")\n",
    "            \n",
    "            # Build the model\n",
    "            arXiv_classification_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(VECTOR_SIZE,)),\n",
    "                #tf.keras.layers.Dense(num_neurons, activation='relu'),\n",
    "                #tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_neurons, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_neurons, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(num_training_categories, activation='softmax')\n",
    "            ])\n",
    "            # Compile the model\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "            # Compile the model & TensorFlow with Metal automatically distribute operations across available GPUs      \n",
    "            arXiv_classification_model.compile(optimizer=optimizer,\n",
    "                        loss='categorical_crossentropy',  # a multi-category classification task expects the targets to be in one-hot encoded format\n",
    "                        metrics=[\n",
    "                            'accuracy',                                      # Basic accuracy\n",
    "                            tf.keras.metrics.Precision(name='precision'),    # Precision metric\n",
    "                            tf.keras.metrics.Recall(name='recall'),          # Recall metric\n",
    "                            CosineSimilarity(name='cosine_similarity')       # Cosine Similarity    \n",
    "                        ])\n",
    "            \n",
    "            # Fitting an ANN model\n",
    "            History = arXiv_classification_model.fit(X_train, y_train, batch_size=2048, epochs=20, verbose=1)\n",
    "            # batch_size = 512 seems to be \"optimal\" for training speed while using gpus with METAL support on Mac\n",
    "            # h5 format is fine, but we should use the native Keras format for saving models\n",
    "            arXiv_classification_model.save(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/v188_ANN_model.keras\")\n",
    "            # In-sample performance metrics\n",
    "            file.write(\"\\nIn sample performance metrics: \\n\")\n",
    "            in_sample_metrics = arXiv_classification_model.evaluate(X_train, y_train, verbose=0)\n",
    "            file.write(f\"Loss: {in_sample_metrics[0]}, Accuracy: {in_sample_metrics[1]}, Precision: {in_sample_metrics[2]}, Recall: {in_sample_metrics[3]}, Cosine Similarity: {in_sample_metrics[4]}\" + \"\\n\")\n",
    "            # Out-of-sample performance metrics\n",
    "            file.write(\"\\nOut of sample performance metrics: \\n\")\n",
    "            out_of_sample_metrics = arXiv_classification_model.evaluate(X_test, y_test, verbose=0)\n",
    "            file.write(f\"Loss: {out_of_sample_metrics[0]}, Accuracy: {out_of_sample_metrics[1]}, Precision: {out_of_sample_metrics[2]}, Recall: {out_of_sample_metrics[3]}, Cosine Similarity: {out_of_sample_metrics[4]}\" + \"\\n\")\n",
    "            #store the out-of-sample accuracy\n",
    "            out_of_sample_accuracies.append(out_of_sample_metrics[1])\n",
    "            # Calculate overfitting ratio = test loss / train loss\n",
    "            overfitting_ratio = out_of_sample_metrics[0] / in_sample_metrics[0]\n",
    "            overfitting_ratios.append(overfitting_ratio)\n",
    "            acceptable_threshold = 1.5\n",
    "            overfitting_status = \"acceptable\" if overfitting_ratio <= acceptable_threshold else \"above threshold\"\n",
    "            file.write(f\"Overfitting Ratio: {overfitting_ratio:.2f}\\n\")\n",
    "            file.write(f\"Overfitting Status: {overfitting_status}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# Write the elapsed time and timestamp to the file\n",
    "with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\", 'a') as file:\n",
    "    file.write(f\"Elapsed time: {elapsed_time / 60} minutes \\n\")\n",
    "    file.write(f\"Timestamp: {current_time}\\n\")\n",
    "# Print the elapsed time and timestamp\n",
    "print(f\"Elapsed time: {elapsed_time / 60} minutes\")\n",
    "print(f\"Timestamp: {current_time} \\n\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Out-of-Sample Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dropout_rates, out_of_sample_accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Out-of-Sample Accuracy')\n",
    "plt.title('Out-of-Sample Accuracy vs. dropout_rates')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Overfitting Ratio\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dropout_rates, overfitting_ratios, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Overfitting Ratio')\n",
    "plt.title('Overfitting Ratio vs. dropout_rates')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON_extracted_vectors = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/X.npy\"\n",
    "VECTOR_SIZE = 300\n",
    "num_neurons = 1000\n",
    "dropout = 0.64\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "    X_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_test.npy')\n",
    "    y_test = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_test.npy')\n",
    "    X_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/X_train.npy')\n",
    "    y_train = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_train.npy')\n",
    "\n",
    "    # Define regularization parameters\n",
    "    l2_regularization_factor = 0.01\n",
    "    \n",
    "    # Train plain vanilla (2 layers) neural networks, a classification model\n",
    "    # Define the model architecture\n",
    "    arXiv_classification_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(VECTOR_SIZE,)),  # matching the Doc2Vec vector size of 100\n",
    "        tf.keras.layers.Dense(num_neurons, activation='relu'),  # 1st hidden layer with 3072 neurons\n",
    "        tf.keras.layers.Dropout(dropout),  # Dropout layer with 50% dropout rate\n",
    "        tf.keras.layers.Dense(num_neurons, activation='relu'),  # 2nd hidden layer with 3072 neurons\n",
    "        tf.keras.layers.Dropout(dropout),  # Dropout layer with 50% dropout rate\n",
    "        #tf.keras.layers.Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_regularization_factor)),  # 3rd hidden layer with 3072 neurons\n",
    "        #tf.keras.layers.Dropout(dropout),  # Dropout layer with 50% dropout rate\n",
    "        #tf.keras.layers.Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_regularization_factor)),  # 4th hidden layer with 3072 neurons\n",
    "        #tf.keras.layers.Dropout(dropout),  # Dropout layer with 50% dropout rate\n",
    "        tf.keras.layers.Dense(num_training_categories, activation='softmax')  # Output layer for num_categories\n",
    "    ])\n",
    "    # TODO: need to test this trick: model.add(tf.keras.layers.Dropout(0, 4))\n",
    "    # model.add(tf.keras.layers.Dropout(0.4)) : 0.4 is the dropout rate, which means 40% of the input units will be randomly set to 0 during training to prevent overfitting.\n",
    "    \n",
    "    # Define the optimizer with a specific learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001) # 0.001 is the default rate\n",
    "    \n",
    "    # Compile the model & TensorFlow with Metal automatically distribute operations across available GPUs      \n",
    "    arXiv_classification_model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',  # a multi-category classification task expects the targets to be in one-hot encoded format\n",
    "                metrics=[\n",
    "                    'accuracy',                                      # Basic accuracy\n",
    "                    tf.keras.metrics.Precision(name='precision'),    # Precision metric\n",
    "                    tf.keras.metrics.Recall(name='recall'),          # Recall metric\n",
    "                    CosineSimilarity(name='cosine_similarity')       # Cosine Similarity    \n",
    "                ])\n",
    "    \n",
    "    # Fitting an ANN model\n",
    "    History = arXiv_classification_model.fit(X_train, y_train, batch_size=2048, epochs=20, verbose=1)\n",
    "    # batch_size = 512 seems to be \"optimal\" for training speed while using gpus with METAL support on Mac\n",
    "\n",
    "    # h5 format is fine, but we should use the native Keras format for saving models\n",
    "    arXiv_classification_model.save(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/ANN_minor_normalized.keras\")\n",
    "    # alternatively\n",
    "    # tf.keras.models.save_model(arXiv_classification_model, f\"{OUTPUT_FILE}_model.keras\")\n",
    "\n",
    "    with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\",'a') as file:\n",
    "        file.write(\"\\n════════════════════════════════════\\n\")\n",
    "        file.write(f\"ANN's performance metrics with 0.001 LR, 2048 batch_size, 40 epochs, 2 hidden layers (each with {num_neurons} neurons with {dropout} dropout).\\n\")\n",
    "        file.write(f\"ANN is trained on inferred embeddings of {VECTOR_SIZE} vector size using titles + abstracts from Cornell huggingface minor dataset. \\n\")\n",
    "        file.write(f\"Doc2vec is trained on processed (titles + abstracts + fulltexts) from minor dataset with epoch = 100, alpha=0.02, min_alpha=0.0001 \\n\")\n",
    "        \n",
    "        # In-sample accuracy and loss metrics\n",
    "        # Multi-class cross-entropy is the chosen objective function \n",
    "        file.write(\"\\nIn sample performance metrics: \\n\")\n",
    "        in_sample_metrics = arXiv_classification_model.evaluate(X_train, y_train)\n",
    "        file.write(f\"Loss: {in_sample_metrics[0]}, Accuracy: {in_sample_metrics[1]}, Precision: {in_sample_metrics[2]}, Recall: {in_sample_metrics[3]}\" + \"\\n\")\n",
    "\n",
    "        # Out-of-sample accuracy and loss metrics\n",
    "        # Multi-class cross-entropy is the chosen objective function \n",
    "        file.write(\"\\nOut of sample performance metrics: \\n\")\n",
    "        out_of_sample_metrics = arXiv_classification_model.evaluate(X_test, y_test)\n",
    "        file.write(f\"Loss: {out_of_sample_metrics[0]}, Accuracy: {out_of_sample_metrics[1]}, Precision: {out_of_sample_metrics[2]}, Recall: {out_of_sample_metrics[3]}\" + \"\\n\")\n",
    "\n",
    "        # Calculate overfitting ratio\n",
    "        overfitting_ratio = out_of_sample_metrics[0] / in_sample_metrics[0]\n",
    "        acceptable_threshold = 1.5\n",
    "        overfitting_status = \"acceptable\" if overfitting_ratio <= acceptable_threshold else \"above threshold\"\n",
    "\n",
    "        # Overfitting evaluation\n",
    "        file.write(\"\\nOverfitting Evaluation:\\n\")\n",
    "        file.write(f\"Overfitting Ratio: {overfitting_ratio:.2f}\\n\")\n",
    "        file.write(f\"Overfitting Status: {overfitting_status} \\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Write the elapsed time and timestamp to the file\n",
    "with open(\"/Users/sro/2024ML-Research/Performance_Metrics/datasets/summary.txt\", 'a') as file:\n",
    "    file.write(f\"Elapsed time: {elapsed_time / 60} minutes \\n\")\n",
    "    file.write(f\"Timestamp: {current_time}\\n\")\n",
    "\n",
    "# Print the elapsed time and timestamp\n",
    "print(f\"Elapsed time: {elapsed_time / 60} minutes\")\n",
    "print(f\"Timestamp: {current_time} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually calculate OOS performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_path = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/v188_ANN_model.keras\"\n",
    "\n",
    "ANN_model = tf.keras.models.load_model(NN_model_path)\n",
    "y_hat_softmax_probabilities = ANN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_softmax_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted probabilities to class labels\n",
    "# returns the INDEX of the highest probability for each sample in y_hat_softmax_probabilities.\n",
    "y_hat_one_hot = np.argmax(y_hat_softmax_probabilities, axis=1)\n",
    "y_hat_one_hot = to_categorical(y_hat_one_hot, num_classes = num_training_categories)\n",
    "print(y_hat_softmax_probabilities.shape)\n",
    "print(y_hat_one_hot.shape)\n",
    "\n",
    "#y_test_one_hot = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_train.npy')\n",
    "y_test_one_hot = np.load('/Users/sro/2024ML-Research/Performance_Metrics/datasets/y_test.npy')\n",
    "print(y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_hat_one_hot, y_test_one_hot)\n",
    "precision = precision_score(y_test_one_hot, y_hat_one_hot, average='weighted')\n",
    "recall = recall_score(y_test_one_hot, y_hat_one_hot, average='weighted')\n",
    "f1 = f1_score(y_test_one_hot, y_hat_one_hot, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "print(f\"Precision: {precision}\\n\")\n",
    "print(f\"Recall: {recall}\\n\")\n",
    "print(f\"F1-Score: {f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Replace with your actual data\n",
    "data = np.array([\n",
    "    [69.7, 1.1],  # X_test_extracted comparisons\n",
    "    [1.5, 58.2]   # X_test_inferred comparisons\n",
    "])\n",
    "\n",
    "# Labels for the axes\n",
    "horizontal_labels = ['X_train_extracted', 'X_train_inferred']\n",
    "vertical_labels = ['X_test_extracted', 'X_test_inferred']\n",
    "\n",
    "# Create the figure and the axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap\n",
    "cax = ax.matshow(data, cmap='coolwarm')\n",
    "\n",
    "# Add color bar\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set axis ticks\n",
    "ax.set_xticks(np.arange(len(horizontal_labels)))\n",
    "ax.set_yticks(np.arange(len(vertical_labels)))\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xticklabels(horizontal_labels)\n",
    "ax.set_yticklabels(vertical_labels)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(vertical_labels)):\n",
    "    for j in range(len(horizontal_labels)):\n",
    "        text = ax.text(j, i, f\"{data[i, j]:.2f}\",\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Training Data')\n",
    "plt.ylabel('Testing Data')\n",
    "\n",
    "# Add title\n",
    "plt.title('Out of sample accuracy for imbalanced (major) dataset')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
