{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a247439",
   "metadata": {},
   "source": [
    "# 1.Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1faeba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sro/2024ML-Research/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# dataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"major\") # imbalanced\\n# dataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"all2023\") # all articles from 2023 only\\ndataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"default\") # minor (balanced)\\nminor_train = dataset[\\'train\\']\\ntrain_categories = minor_train[\\'primary_subfield\\']\\nminor_test = dataset[\\'test\\']\\ntest_categories = minor_test[\\'primary_subfield\\']\\n\\n# Extract features and labels\\ntexts = [item[\\'text\\'] for item in dataset]\\nlabels = [item[\\'label\\'] for item in dataset]  # Adjust the key as per the dataset\\'s structure\\n# Split the dataset into training and testing sets\\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=1, shuffle=True)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "'''\n",
    "# dataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"major\") # imbalanced\n",
    "# dataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"all2023\") # all articles from 2023 only\n",
    "dataset = load_dataset(\"mlcore/arxiv-classifier\", name=\"default\") # minor (balanced)\n",
    "minor_train = dataset['train']\n",
    "train_categories = minor_train['primary_subfield']\n",
    "minor_test = dataset['test']\n",
    "test_categories = minor_test['primary_subfield']\n",
    "\n",
    "# Extract features and labels\n",
    "texts = [item['text'] for item in dataset]\n",
    "labels = [item['label'] for item in dataset]  # Adjust the key as per the dataset's structure\n",
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=1, shuffle=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbfc9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of minor train set rows: 108,696\n",
      "\n",
      "Total number of minor test set rows: 27,178\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf_minor = pd.concat([df_minor_train, df_minor_test], axis=0, ignore_index=True)\\ndf_minor.to_json(\\'/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor.json\\', orient=\\'records\\', lines=True)\\n\\nJSON_10 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_processed.json\"\\ndf_minor = pd.read_json(JSON_10, lines = \"True\")\\nformatted_num_rows = \"{:,}\".format(len(df_minor))\\nprint(f\"Total number of minor set rows: {formatted_num_rows}\\n\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON_8 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_train_processed.json\"\n",
    "JSON_9 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_test_processed.json\"\n",
    "df_minor_train = pd.read_json(JSON_8, lines = \"True\")\n",
    "df_minor_test = pd.read_json(JSON_9, lines = \"True\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_minor_train))\n",
    "print(f\"Total number of minor train set rows: {formatted_num_rows}\\n\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_minor_test))\n",
    "print(f\"Total number of minor test set rows: {formatted_num_rows}\\n\")\n",
    "\n",
    "'''\n",
    "df_minor = pd.concat([df_minor_train, df_minor_test], axis=0, ignore_index=True)\n",
    "df_minor.to_json('/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor.json', orient='records', lines=True)\n",
    "\n",
    "JSON_10 = \"/Users/sro/2024ML-Research/Performance_Metrics/datasets/minor_processed.json\"\n",
    "df_minor = pd.read_json(JSON_10, lines = \"True\")\n",
    "formatted_num_rows = \"{:,}\".format(len(df_minor))\n",
    "print(f\"Total number of minor set rows: {formatted_num_rows}\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e331297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If splitting the DataFrame into training and test sets, reset the indices to ensure they are continuous and match correctly.\n",
    "df_minor_train = df_minor_train.reset_index(drop=True)\n",
    "\n",
    "categories = df_minor_train['primary_subfield']\n",
    "primary_categories_train = [category.split()[0] for category in categories]\n",
    "fulltexts_train = df_minor_train['fulltext']\n",
    "\n",
    "titles_and_abstracts_train = []\n",
    "abstracts = [sentence for sentence in df_minor_train['abstract']]\n",
    "titles = [sentence for sentence in df_minor_train['title']]\n",
    "for title, abstract in zip(titles, abstracts):\n",
    "    combined = title + abstract\n",
    "    titles_and_abstracts_train.append(combined)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(primary_categories_train)\n",
    "y_train_integer = label_encoder.transform(primary_categories_train)\n",
    "num_categories = len(label_encoder.classes_)\n",
    "#print(label_encoder.classes_)\n",
    "# y_one_hot = to_categorical(y_integer, num_classes = num_categories)\n",
    "\n",
    "categories = df_minor_test['primary_subfield']\n",
    "primary_categories_test = [category.split()[0] for category in categories]\n",
    "y_test_integer = label_encoder.transform(primary_categories_test)\n",
    "fulltexts_test = df_minor_test['fulltext']\n",
    "\n",
    "titles_and_abstracts_test = []\n",
    "abstracts = [sentence for sentence in df_minor_test['abstract']]\n",
    "titles = [sentence for sentence in df_minor_test['title']]\n",
    "for title, abstract in zip(titles, abstracts):\n",
    "    combined = title + abstract\n",
    "    titles_and_abstracts_test.append(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869fb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astro-ph' 'astro-ph.CO' 'astro-ph.EP' 'astro-ph.GA' 'astro-ph.HE'\n",
      " 'astro-ph.IM' 'astro-ph.SR' 'cond-mat.dis-nn' 'cond-mat.mes-hall'\n",
      " 'cond-mat.mtrl-sci' 'cond-mat.other' 'cond-mat.quant-gas' 'cond-mat.soft'\n",
      " 'cond-mat.stat-mech' 'cond-mat.str-el' 'cond-mat.supr-con' 'cs.AI'\n",
      " 'cs.AR' 'cs.CC' 'cs.CE' 'cs.CG' 'cs.CL' 'cs.CR' 'cs.CV' 'cs.CY' 'cs.DB'\n",
      " 'cs.DC' 'cs.DL' 'cs.DM' 'cs.DS' 'cs.ET' 'cs.FL' 'cs.GL' 'cs.GR' 'cs.GT'\n",
      " 'cs.HC' 'cs.IR' 'cs.IT' 'cs.LG' 'cs.LO' 'cs.MA' 'cs.MM' 'cs.MS' 'cs.NE'\n",
      " 'cs.NI' 'cs.OH' 'cs.OS' 'cs.PF' 'cs.PL' 'cs.RO' 'cs.SC' 'cs.SD' 'cs.SE'\n",
      " 'cs.SI' 'econ.EM' 'econ.TH' 'eess.AS' 'eess.IV' 'eess.SP' 'eess.SY'\n",
      " 'gr-qc' 'hep-ex' 'hep-lat' 'hep-ph' 'hep-th' 'math-ph' 'math.AC'\n",
      " 'math.AG' 'math.AP' 'math.AT' 'math.CA' 'math.CO' 'math.CT' 'math.CV'\n",
      " 'math.DG' 'math.DS' 'math.FA' 'math.GN' 'math.GR' 'math.GT' 'math.HO'\n",
      " 'math.KT' 'math.LO' 'math.MG' 'math.NA' 'math.NT' 'math.OA' 'math.OC'\n",
      " 'math.PR' 'math.QA' 'math.RA' 'math.RT' 'math.SG' 'math.SP' 'math.ST'\n",
      " 'nlin.AO' 'nlin.CD' 'nlin.CG' 'nlin.PS' 'nlin.SI' 'nucl-ex' 'nucl-th'\n",
      " 'physics.acc-ph' 'physics.ao-ph' 'physics.app-ph' 'physics.atm-clus'\n",
      " 'physics.atom-ph' 'physics.bio-ph' 'physics.chem-ph' 'physics.class-ph'\n",
      " 'physics.comp-ph' 'physics.data-an' 'physics.ed-ph' 'physics.flu-dyn'\n",
      " 'physics.geo-ph' 'physics.hist-ph' 'physics.ins-det' 'physics.med-ph'\n",
      " 'physics.optics' 'physics.plasm-ph' 'physics.pop-ph' 'physics.soc-ph'\n",
      " 'physics.space-ph' 'q-bio.BM' 'q-bio.CB' 'q-bio.GN' 'q-bio.MN' 'q-bio.NC'\n",
      " 'q-bio.PE' 'q-bio.QM' 'q-bio.SC' 'q-bio.TO' 'q-fin.CP' 'q-fin.GN'\n",
      " 'q-fin.MF' 'q-fin.PM' 'q-fin.PR' 'q-fin.RM' 'q-fin.ST' 'q-fin.TR'\n",
      " 'quant-ph' 'stat.AP' 'stat.CO' 'stat.ME' 'stat.ML']\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_eval_1, y_train_1, y_eval_1 = train_test_split(fulltexts_train, y_train_integer, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = pd.DataFrame({'fulltext': fulltexts_train, 'title_abstract': titles_and_abstracts_train, 'label': y_train_integer})\n",
    "test_data = pd.DataFrame({'fulltext': fulltexts_test, 'title_abstract': titles_and_abstracts_test, 'label': y_test_integer})\n",
    "\n",
    "train_data.to_json('/Users/sro/2024ML-Research/MPNet/minor_train.json', orient='records', lines=True)\n",
    "test_data.to_json('/Users/sro/2024ML-Research/MPNet/minor_test.json', orient='records', lines=True)\n",
    "\n",
    "print(\"Train and test data saved as JSON files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba9b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Check if index {idx} is valid.\")\n",
    "            raise\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "JSON_1 = '/Users/sro/2024ML-Research/MPNet/minor_train.json'\n",
    "JSON_2 = '/Users/sro/2024ML-Research/MPNet/minor_test.json'\n",
    "df_minor_train = pd.read_json(JSON_1, lines = \"True\")\n",
    "df_minor_test = pd.read_json(JSON_2, lines = \"True\")\n",
    "X_train = df_minor_train['fulltext'] # could use title + abstract for vector inference instead of full text\n",
    "y_train = df_minor_train['label']\n",
    "num_categories = len(set(y_train))\n",
    "X_test = df_minor_test['fulltext'] # could use title + abstract for vector inference instead of full text\n",
    "y_test = df_minor_test['label']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "train_dataset = ArxivDataset(X_train, y_train, tokenizer, max_length=512) #  a maximum sequence length of tokens as context window is set here\n",
    "test_dataset = ArxivDataset(X_test, y_test, tokenizer, max_length=512) # 512 is usally by default\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "#DataCollatorWithPadding for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ca30ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 16857,  2968,  1015, 10576,  5298, 16105,  6309, 29241,  2002,\n",
       "         24313, 18423,  2003,  2000,  3306,  2114,  2001, 13679,  1015,  2083,\n",
       "          5673, 12174,  2479,  3370,  2513,  1015, 12174, 24318, 24313, 16025,\n",
       "         20354,  2019,  1043,  1016,  1015,  1043,  1016,  8806,  2491,  1014,\n",
       "          1020,  1014,  1053,  1016,  1052,  1016,  5576,  9081,  2491,  1014,\n",
       "          1020,  1014,  1052,  1016,  2006,  2513,  1014,  1064,  1016, 12853,\n",
       "          2230,  2513,  1014,  1053,  1016, 11378,  2513,  1014,  1063,  1016,\n",
       "          5474,  2491,  1014,  1054,  1016, 19137,  2491,  1014,  1051,  1016,\n",
       "          1015,  1052,  1016,  7422,  2513,  2002,  1054,  1016,  1015,  1043,\n",
       "          1016,  6304,  2236,  2491,  1014,  1020,  1014,  1022,  1012,  1019,\n",
       "          2537,  2001,  5588,  1014,  2666,  2824,  2001,  2978,  1014, 18884,\n",
       "          1014,  6191, 19993, 17792,  1014,  3919,  1020,  2824,  2009,  8563,\n",
       "          2596,  2002,  3047,  1014,  2666,  2824,  2001,  2978,  1014, 18884,\n",
       "          1014,  6191, 19993, 17792,  1014,  3919,  1021,  2537,  2001,  5996,\n",
       "          3334,  1014,  2122,  2001,  2666,  1014,  3054,  3353,  1014,  6191,\n",
       "          7710,  2687,  2633,  1014,  3919,  1022, 10560,  2619,  3673, 28995,\n",
       "         11024, 13688,  2824,  1014,  2666,  2824,  2001,  2978,  1014, 18884,\n",
       "          1014,  6191, 19993, 17792,  1014,  3919,  1012,  1045,  1015,  5657,\n",
       "          1028, 13320, 25677,  1034, 10254, 15011,  1016,  3972,  2230, 10065,\n",
       "          1016, 10043,  1015, 10576,  8064,  3900,  2010,  2000,  3306, 16105,\n",
       "          6309, 29241,  2001, 24313, 16025, 20354,  2019,  2028, 10851,  2015,\n",
       "         13726,  5238,  2079, 28703, 13185, 24899,  1010,  8545,  1011,  2917,\n",
       "          2001, 12174, 24318,  5094,  5402,  2001, 25676,  5673, 12174,  2479,\n",
       "          3370,  2513,  4861,  3156,  2010,  2331,  2001, 13679,  1015,  2083,\n",
       "          5673, 12174,  2479,  3370,  2513,  9018,  1016,  2009, 14987,  2001,\n",
       "          2000,  2331, 12174,  2479,  3370,  2513,  6745,  5024,  2004,  2034,\n",
       "          3764,  2088,  1021, 21868,  3376,  6283,  2575,  9018,  1010,  1057,\n",
       "          2144,  1011,  1014,  1041, 13593,  2139,  2003, 23397, 23928, 14773,\n",
       "          3306, 17439,  6582,  7484,  2043,  2921,  1041,  8285,  4864, 22979,\n",
       "         14138,  1014,  2033,  2007,  2176,  3024,  2088,  2000,  9629, 12735,\n",
       "          7377,  7609,  4344,  2017,  2000,  2023,  9630, 15538,  2538,  5016,\n",
       "          1016,  2000,  2816,  3647,  2002, 13593,  4864, 22979, 24008,  6918,\n",
       "         20179,  2104,  2001,  2000,  6582,  3231,  3627,  2011,  4856,  1043,\n",
       "          1015,  8127,  8064,  2496,  1010,  1048,  1011,  2002,  4856, 13679,\n",
       "         23803,  2508,  1010,  1064,  1011,  1014,  9108,  2012,  2000,  3562,\n",
       "          4765,  2001,  2027,  3306,  6582,  2007,  3382,  2011, 10043,  1015,\n",
       "         10576,  1043,  1015,  8127, 10772, 18937,  2867, 10181, 17460,  1016,\n",
       "          2010,  2000,  2064,  2196,  1014,  2000,  4864,  1010,  1060,  1011,\n",
       "         18646,  2001,  2007,  2516,  1015, 18851, 25013,  1014,  4764,  3992,\n",
       "          3627,  2921, 22979, 14138,  1014,  2033,  2007,  2632,  2015,  1041,\n",
       "          1527, 16514,  1528,  2002,  2063,  9470,  7609,  1016,  2126, 13356,\n",
       "          2093,  2026,  7112,  2004, 10043, 16857,  2968,  2157,  1014,  4289,\n",
       "          4559,  2016,  1060,  1030,  1030, 22979, 10576,  2015,  2052,  4131,\n",
       "          2001,  5861,  2011,  2371,  4864, 18646,  2019,  1028,  1041,  2097,\n",
       "          1015,  8793,  6695,  2017,  2000,  9629, 16857,  2968,  2012, 29536,\n",
       "          2016,  2663,  1060,  1014,  2002,  1041,  2052,  1015,  8793,  6695,\n",
       "          3382,  2011,  2000,  1058, 19662,  2104, 10270, 19876,  2015,  3306,\n",
       "         16105,  6309, 10772, 28554,  5648,  1014,  2033, 29536,  2016, 22979,\n",
       "          7609, 14138,  1016,  2003,  2808,  2004,  2000,  5163, 10043, 16857,\n",
       "          2968,  1014, 13593,  2139, 22578,  4633, 24505,  7860,  3376,  1030,\n",
       "          1030,  1060,  1030, 22979, 29241,  2028,  2183,  2251,  2000,  7376,\n",
       "          2001,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(8)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95086a",
   "metadata": {},
   "source": [
    "# 2.Fine-Tune the MPNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cd3f2",
   "metadata": {},
   "source": [
    "training a classification head and update some weights of MPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08231e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') cuda does not work on Mac\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/all-mpnet-base-v2', num_labels=num_categories)\n",
    "\n",
    "# recommended -> default option, i.e., the weights of both the pretrained model and the classification head will be updated\n",
    "\n",
    "# Freeze all layers of the pretrained model and only the classification head will be fine-tuned\n",
    "#for param in model.base_model.parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/sro/2024ML-Research/MPNet/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/Users/sro/2024ML-Research/MPNet/logs',\n",
    "    logging_steps=100,  \n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    #dataloader_num_workers=4\n",
    "    #fp16=True #fp16 mixed precision requires a GPU (not 'mps').\n",
    ")\n",
    "    \n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_model_fine_tuned') # save the enitre model including the new head\n",
    "tokenizer.save_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc86745",
   "metadata": {},
   "source": [
    "Using MPNet alone to make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca1f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 12:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7247\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_tokenizer_6_epochs_minor')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_model_fine_tuned_6_epochs_minor')\n",
    "# Must load AutoModelForSequenceClassification instead of AutoModel to put back the classification head we have trained!!\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/sro/2024ML-Research/MPNet/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/Users/sro/2024ML-Research/MPNet/logs',\n",
    "    logging_steps=100,  \n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99bc789",
   "metadata": {},
   "source": [
    "# 3. Compute Embeddings Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e91ce",
   "metadata": {},
   "source": [
    "cell below contains alternative workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32480536",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_tokenizer_6_epochs_minor')\n",
    "model = AutoModel.from_pretrained('/Users/sro/2024ML-Research/MPNet/fine_tuned_mpnet/MPNet_model_fine_tuned_6_epochs_minor')\n",
    "#for embeddings: Use AutoModel\n",
    "#for classification tasks: Use AutoModelForSequenceClassification\n",
    "\n",
    "#ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Use DataParallel for multi-GPU processing\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    print(f\"Using {torch.cuda.device_count()} GPUs for processing\")\n",
    "#    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def batched(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while (batch := list(islice(it, n))):\n",
    "        yield batch\n",
    "        \n",
    "#function to compute embeddings\n",
    "def compute_embeddings(texts, batch_size=16):\n",
    "    dataloader = DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "            model_output = model(**encoded_input)\n",
    "            mean_pooled = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            normalized_embeddings = F.normalize(mean_pooled, p=2, dim=1)\n",
    "            embeddings.append(normalized_embeddings.cpu().numpy())\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "train_embeddings = compute_embeddings(X_train)\n",
    "test_embeddings = compute_embeddings(X_test)\n",
    "\n",
    "np.save('/Users/sro/2024ML-Research/MPNet/MPNet_train_embeddings_fulltext_6epochs_minor.npy', train_embeddings)\n",
    "np.save('/Users/sro/2024ML-Research/MPNet/MPNet_test_embeddings_fulltext_6epochs_minor.npy', test_embeddings)\n",
    "\n",
    "print(\"Embeddings computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e1ea6",
   "metadata": {},
   "source": [
    "# 4. Loading the Embeddings and Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8775fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = np.load('/Users/sro/2024ML-Research/MPNet/MPNet_train_embeddings_fulltext_6epochs_minor.npy')\n",
    "X_test_embeddings = np.load('/Users/sro/2024ML-Research/MPNet/MPNet_test_embeddings_fulltext_6epochs_minor.npy')\n",
    "\n",
    "# y_train_one_hot = to_categorical(y_train, num_classes = num_categories)\n",
    "# y_test_one_hot = to_categorical(y_test, num_classes = num_categories)\n",
    "\n",
    "#convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_embeddings, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_embeddings, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "#create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de763a",
   "metadata": {},
   "source": [
    "# 5. Defining the Artificial Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters to fine-tune\n",
    "hidden_dims = [512, 1024, 2048]\n",
    "num_epochs_list = [20, 30, 40]\n",
    "dropout_prob = 0.5\n",
    "input_dim = X_train_embeddings.shape[1]\n",
    "output_dim = len(label_encoder.classes_)\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(hidden_dim, num_epochs):\n",
    "    # Initialize the model\n",
    "    model = SimpleNN(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Store accuracy values\n",
    "    accuracy_values = []\n",
    "\n",
    "    # Training the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_hat = predicted.cpu().numpy()\n",
    "            y_true = y_test_tensor.cpu().numpy()\n",
    "            accuracy = accuracy_score(y_true, y_hat)\n",
    "            accuracy_values.append(accuracy)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_true, y_hat, target_names=label_encoder.classes_))\n",
    "\n",
    "    return accuracy_values\n",
    "\n",
    "# Perform grid search and plot accuracy\n",
    "results = {}\n",
    "for hidden_dim, num_epochs in itertools.product(hidden_dims, num_epochs_list):\n",
    "    print(f'Training with Hidden Dim: {hidden_dim}, Epochs: {num_epochs}')\n",
    "    accuracy_values = train_and_evaluate_model(hidden_dim, num_epochs)\n",
    "    results[(hidden_dim, num_epochs)] = accuracy_values\n",
    "\n",
    "# Plot accuracy vs. epochs for each hidden dimension\n",
    "for (hidden_dim, num_epochs), accuracies in results.items():\n",
    "    plt.plot(range(1, num_epochs + 1), accuracies, label=f'Hidden Dim: {hidden_dim}, Epochs: {num_epochs}')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fe57e",
   "metadata": {},
   "source": [
    "# 6. Training the ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "dropout_prob = 0.5\n",
    "input_dim = X_train_embeddings.shape[1]\n",
    "output_dim = len(label_encoder.classes_)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "    \n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ba941",
   "metadata": {},
   "source": [
    "# 7. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_hat = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    accuracy = accuracy_score(y_true, y_hat)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print(classification_report(y_true, y_hat, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
